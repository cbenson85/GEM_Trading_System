# GEM Trading System - AI Catch-Up Prompt
**Last Updated**: 2025-11-02 18:45:00
**System Version**: 5.0.2-SUSTAINABILITY-FILTER
**Current Phase**: Phase 3: Stock Discovery & Pre-Catalyst Analysis - SUSTAINABILITY TESTING

---

## üö® CRITICAL REMINDERS FOR AI - READ THIS FIRST

### 1. FILE CATALOG MUST BE UPDATED FOR EVERY FILE CREATION - NO EXCEPTIONS!

When AI creates ANY file:
1. ‚úÖ Create file with download link
2. ‚úÖ **IMMEDIATELY UPDATE GITHUB_FILE_CATALOG.md with ‚è≥ PENDING status**
3. ‚úÖ Post GitHub URL for user to copy
4. ‚úÖ Wait for user to upload and paste back
5. ‚úÖ Verify file via web_fetch
6. ‚úÖ **UPDATE GITHUB_FILE_CATALOG.md with ‚úÖ VERIFIED status**
7. ‚úÖ Update CURRENT_CATCHUP_PROMPT.md if major milestone

**This is MANDATORY. Not optional. Not negotiable. ALWAYS.**

See [FILE_VERIFICATION_PROTOCOL.md](https://raw.githubusercontent.com/cbenson85/GEM_Trading_System/refs/heads/main/FILE_VERIFICATION_PROTOCOL.md) for complete workflow.

### 2. CATCH-UP PROMPT + SYSTEM_STATE.JSON ARE PAIRED FILES - UPDATE BOTH!

**When CURRENT_CATCHUP_PROMPT.md is updated, system_state.json MUST be updated too!**

These files work together:
- **CURRENT_CATCHUP_PROMPT.md** = Human-readable narrative
- **system_state.json** = Machine-readable structured data

They must ALWAYS have:
- ‚úÖ Matching timestamps
- ‚úÖ Matching current phase/status
- ‚úÖ Matching next steps
- ‚úÖ Same key information

**Never update one without the other!**

---

## üéØ CRITICAL CONTEXT

### What You're Walking Into
System has completed Phase 2 infrastructure. Sustainability filter V2.0 has been developed and optimized for Polygon Developer tier (unlimited API). Filter is about to run to test all 200 stocks for 21-day, 80% retention sustainability.

### Immediate Priority
Sustainability filter workflow is running RIGHT NOW. After completion, analyze results and proceed with pre-catalyst data collection for sustainable stocks only.

---

## üìä SYSTEM STATUS

### Current State
- **Portfolio**: CLEARED - Starting fresh
- **Cash Available**: $10,000 (reset)
- **System Stage**: Phase 3 - Sustainability Filter Running
- **Last Completed Phase**: Phase 2: System Requirements & Data Infrastructure - COMPLETE ‚úÖ
- **Current Task**: Running sustainability filter V2.0 on 200 stocks
- **Next Phase**: Pre-catalyst analysis on sustainable stocks only

### Data Verification Status

- Current Prices: ‚úÖ VERIFIED (Polygon API)
- Volume Data: ‚úÖ VERIFIED (Polygon API)
- Float Data: ‚ö†Ô∏è PARTIAL (Polygon API - not all stocks)
- Explosive Stock Catalog: ‚úÖ VERIFIED (200 stocks, 2010-2024 excluding COVID)
- Backtest Infrastructure: ‚úÖ COMPLETE
- Data Backup System: ‚úÖ FIXED (merge logic implemented)
- Pre-Catalyst Framework: ‚úÖ CREATED & VERIFIED
- Sustainability Filter: ‚è≥ RUNNING NOW (V2.0 with Developer tier optimization)
- Historical Analysis: ‚è≥ PENDING (starts after sustainability results)

---

## üóÇÔ∏è FILE STRUCTURE & LOCATIONS

**Repository**: https://github.com/cbenson85/GEM_Trading_System

### Core System Files (VERIFIED)

- `/Current_System/GEM_v5_Master_Screening_Protocol.md` - Core screening rules
- `/Current_System/Trading_Rules_Complete.md` - Trading operations guide
- `/FILE_VERIFICATION_PROTOCOL.md` - File access methodology (MANDATORY)
- `/COVID_ERA_EXCLUSION_RULE.md` - Critical COVID-era filter rule

### Data Files (VERIFIED - Location & Status)

- `/Verified_Backtest_Data/explosive_stocks_catalog.json` - ‚úÖ Latest scan results
- `/Verified_Backtest_Data/explosive_stocks_CLEAN.json` - ‚úÖ **200 stocks** (pre-sustainability filter)
- `/Verified_Backtest_Data/explosive_stocks_COVID_ERA.json` - ‚úÖ 81 COVID-era stocks (archived)
- `/Verified_Backtest_Data/explosive_stocks_UNSUSTAINABLE.json` - ‚è≥ BEING CREATED (sustainability filter output)
- `/Verified_Backtest_Data/sustainability_summary.json` - ‚è≥ BEING CREATED (filter statistics)
- `/Verified_Backtest_Data/refinement_history.json` - ‚úÖ System evolution log
- `/Verified_Backtest_Data/README.md` - ‚úÖ Data quality standards
- `/Verified_Backtest_Data/filter_summary.json` - ‚úÖ Merge statistics
- `/Verified_Backtest_Data/PRE_CATALYST_ANALYSIS_FRAMEWORK.md` - ‚úÖ VERIFIED
- `/Verified_Backtest_Data/STANDARD_DATA_FORMAT.md` - ‚úÖ VERIFIED
- `/Verified_Backtest_Data/PHASE_3_IMPLEMENTATION_PLAN.md` - ‚úÖ VERIFIED
- `/Verified_Backtest_Data/pre_catalyst_analysis/` - ‚ùå TO BE CREATED (Phase 3 data)
- `/Verified_Backtest_Data/backtest_runs/` - ‚ùå TO BE CREATED (Phase 4+)

### Active Scripts (VERIFIED)

- `/explosive_stock_scanner.py` - ‚úÖ Multi-year scanner (GitHub Actions)
- `/filter_covid_era.py` - ‚úÖ COVID filter WITH MERGE LOGIC
- `/filter_sustainability.py` - ‚úÖ V2.0 (Developer tier optimized) - **RUNNING NOW**
- `/.github/workflows/explosive_stock_scan_workflow.yml` - ‚úÖ Automated scanning
- `/.github/workflows/sustainability_filter_workflow.yml` - ‚úÖ V2.0 (Developer tier) - **RUNNING NOW**

### Archived Content (DO NOT USE FOR DECISIONS)

- `/Archive_Unverified/` - ‚úÖ Contains old unverified data (reference only)
- `/Backtest_Results/` - ‚ö†Ô∏è UNVERIFIED (framework reference only)
- `/Strategy_Evolution/` - ‚ö†Ô∏è UNVERIFIED (concepts only, not data)

### üìÇ Complete GitHub File Catalog

**Full catalog with all links**: [GITHUB_FILE_CATALOG.md](https://raw.githubusercontent.com/cbenson85/GEM_Trading_System/refs/heads/main/GITHUB_FILE_CATALOG.md)

**Critical files for AI:**
- [CURRENT_CATCHUP_PROMPT.md](https://raw.githubusercontent.com/cbenson85/GEM_Trading_System/refs/heads/main/System_State/CURRENT_CATCHUP_PROMPT.md) - This file
- [system_state.json](https://raw.githubusercontent.com/cbenson85/GEM_Trading_System/refs/heads/main/System_State/system_state.json) - System data
- [FILE_VERIFICATION_PROTOCOL.md](https://raw.githubusercontent.com/cbenson85/GEM_Trading_System/refs/heads/main/FILE_VERIFICATION_PROTOCOL.md) - File access method

---

## üî¨ BACKTESTING METHODOLOGY

### Approach

1. ‚úÖ Find stocks with 500%+ gains in any 6-month window (2010-2024) - COMPLETE
2. ‚è≥ Test for sustainability (80% retention, 21 days after peak) - **RUNNING NOW**
3. ‚è≥ Deep dive into 180 days PRE-CATALYST for sustainable stocks - Phase 3
4. ‚ùå Analyze: price, volume, sentiment, leadership, news, patterns - Phase 3
5. ‚ùå Identify correlations between explosive stocks - Phase 3
6. ‚ùå Build screener based on correlations - Phase 3+
7. ‚ùå Backtest on random historical dates - Phase 4+
8. ‚ùå Apply FALSE MISS principle to discarded stocks - Phase 4+
9. ‚ùå Track picks AND discards with full performance data - Phase 4+
10. ‚ùå Refine until consistent 10-year performance - Phase 4+
11. ‚úÖ Store ALL data for future refinement - Infrastructure ready

### Current Progress

**Phase 1**: Discovery & Documentation Audit - ‚úÖ COMPLETE
**Phase 2**: System Requirements & Data Infrastructure - ‚úÖ COMPLETE
- ‚úÖ Multi-year explosive stock scan complete (200 stocks from 2010-2024)
- ‚úÖ COVID-era filter applied (200 clean, 81 archived)
- ‚úÖ File verification protocol established
- ‚úÖ Data backup system fixed (merge logic)
- ‚úÖ All infrastructure verified and working
- ‚úÖ Pre-catalyst analysis framework created (12 categories, 100+ data points)
- ‚úÖ GitHub Actions workflow fixed (full automation, no manual steps)
- ‚úÖ Standard data format template created

**Phase 3**: Stock Discovery & Pre-Catalyst Analysis - ‚è≥ IN PROGRESS
- ‚è≥ **Sustainability filter V2.0 RUNNING NOW** (80% retention, 21 days)
- ‚ùå Analyze sustainability results
- ‚ùå Build data collection scripts for sustainable stocks
- ‚ùå Collect 180-day pre-catalyst data for sustainable stocks
- ‚ùå Build pattern scanning/correlation tools
- ‚ùå Analyze patterns and extract screening criteria
- ‚ùå Document all discoveries

### Data Sources

- Price/Volume: Polygon API (Developer tier - UNLIMITED requests)
- News/Sentiment: Web scraping (Google, Yahoo Finance)
- Insider Trading: Free sources (Finviz, OpenInsider, SEC Form 4)
- SEC Filings: SEC EDGAR (free)
- Float/Shares: Polygon API + manual verification

---

## üî¨ SUSTAINABILITY FILTER V2.0

### Filter Criteria (FINALIZED)
- **Test Window**: 21 days after peak (not 30)
- **Retention Threshold**: 80% of peak gain (not 90%)
- **Test Method**: Use Polygon API to find actual catalyst windows

### How It Works
1. **Read CLEAN.json** (200 stocks to test)
2. **For each stock:**
   - Use Polygon API to scan stock's year for explosive window
   - Find 180-day window with 500%+ gain
   - Get catalyst start date (entry date)
   - Get peak price (highest in window)
   - Get test price (21 days after peak)
   - Calculate: `retention = (test_price - entry_price) / (peak_price - entry_price)`
3. **Filter results:**
   - Sustainable (‚â•80%): Stay in CLEAN.json
   - Unsustainable (<80%): Move to UNSUSTAINABLE.json

### Performance (Developer Tier)
- **API Access**: Unlimited requests (Developer tier)
- **Rate Limiting**: Minimal (0.1s delay between stocks)
- **Expected Runtime**: 5-15 minutes for 200 stocks ‚ö°
- **Not 2-4 hours. Not 8-13 hours. MINUTES!**

### Expected Results
- **Sustainable stocks**: ~60-120 (30-60% of original 200)
- **Unsustainable stocks**: ~80-140 (pump & dumps, untradeable flukes)
- All data preserved in respective files
- Detailed statistics in sustainability_summary.json

### Files Created by Filter
1. **explosive_stocks_CLEAN.json** - MODIFIED (sustainable stocks only)
2. **explosive_stocks_UNSUSTAINABLE.json** - CREATED (pump & dumps)
3. **sustainability_summary.json** - CREATED (statistics)

---

## üìà DISCOVERED PATTERNS

### Verified Correlations
NONE YET - Will be discovered during Phase 3 analysis of SUSTAINABLE stocks only

### Current Scoring Criteria
CURRENT v5.0.2 CRITERIA - Will be updated based on Phase 3 pattern discovery from sustainable stocks

### Refinement History
4 refinements logged:
1. System reset (2025-11-01) - Cleared unverified data
2. COVID-era exclusion rule (2025-11-01) - Exclude 2020-2021 from patterns
3. Expanded historical coverage (2025-11-02) - Added 2010-2012 data (11 stocks)
4. Sustainability filter V2.0 (2025-11-02) - 21-day, 80% retention test

---

## üéØ DECISIONS MADE

### Key Decisions
1. ‚úÖ Mark all previous backtest results as UNVERIFIED
2. ‚úÖ Clear all current portfolio positions - starting fresh
3. ‚úÖ Build from verified data only - no fabrication
4. ‚úÖ Use 500%+ in 6 months as explosive stock criteria
5. ‚úÖ Analyze 180 days pre-catalyst for pattern discovery
6. ‚úÖ Store all backtest data (picks AND discards) for refinement
7. ‚úÖ Apply false miss principle in all backtests
8. ‚úÖ User only does copy/paste - full automation required
9. ‚úÖ Free data sources only (no paid APIs except Polygon)
10. ‚úÖ Exclude COVID-era (2020-2021) from pattern analysis
11. ‚úÖ Implement merge logic in filter to prevent data loss
12. ‚úÖ Fix GitHub Actions workflow for full automation (no manual filter runs)
13. ‚úÖ Expand historical coverage back to 2010 for more data
14. ‚úÖ Create comprehensive pre-catalyst analysis framework (12 categories)
15. ‚úÖ Test sustainability BEFORE pre-catalyst analysis (filter out pump & dumps)
16. ‚úÖ Use 21-day, 80% retention as sustainability criteria
17. ‚úÖ Only analyze sustainable stocks for pattern discovery

### Rules Established
1. **NO FABRICATION** - Verify or say you can't
2. **FALSE MISS CHECK** - Always check discarded stocks for explosive growth
3. **STORE EVERYTHING** - All decisions, data, refinements must be saved
4. **COPY/PASTE ONLY** - User should never manually enter data
5. **10-YEAR VALIDATION** - System must prove consistency before live trading
6. **FILE VERIFICATION** - Every file must be verified using the established protocol
7. **DATA PRESERVATION** - Filter script uses merge logic (no data loss on subsequent scans)
8. **FULL AUTOMATION** - Workflow handles scanning, filtering, and committing without manual steps
9. **SUSTAINABILITY FIRST** - Test for tradeability before spending weeks on analysis
10. **DEVELOPER TIER OPTIMIZED** - Use unlimited API access for fast results

---

## üìù WHAT NEEDS TO HAPPEN NEXT

### Immediate Next Steps (After Filter Completes)

**Step 1: Analyze Sustainability Results** ‚è≥ WAITING FOR FILTER
1. Check sustainability_summary.json statistics
2. Verify CLEAN.json contains only sustainable stocks
3. Review UNSUSTAINABLE.json for patterns
4. Document sustainability rate and findings

**Step 2: Begin Pre-Catalyst Analysis** (After Step 1)
1. Build data collection scripts for sustainable stocks:
   - Price/volume collector (Polygon API)
   - SEC filings collector (EDGAR)
   - Insider trading collector (Form 4, OpenInsider)
   - News/sentiment collector (web scraping)
   - Float/ownership collector (Polygon + Finviz)
   - Financial metrics collector (Polygon + SEC)
2. Collect 180-day pre-catalyst data for sustainable stocks
3. Build pattern scanning/correlation tools
4. Analyze patterns and extract screening criteria
5. Document all findings

### Blockers/Questions
NONE - Filter is running, waiting for completion

---

## üîó IMPORTANT LINKS

- **GitHub Repo**: https://github.com/cbenson85/GEM_Trading_System
- **Polygon API**: Developer tier (UNLIMITED requests, key: pvv6DNmKAoxojCc0B5HOaji6I_k1egv0)
- **Data Storage**: /Verified_Backtest_Data/ (active and verified)
- **File Catalog**: [GITHUB_FILE_CATALOG.md](https://raw.githubusercontent.com/cbenson85/GEM_Trading_System/refs/heads/main/GITHUB_FILE_CATALOG.md)

---

## üîê FILE VERIFICATION PROTOCOL (MANDATORY)

### **The Problem We Solved:**
AI was creating files without verifying it could read them back from GitHub. This created broken links and lost context between sessions.

### **The Solution - REQUIRED WORKFLOW:**

**When AI Creates New File(s):**

**Step 1**: AI provides files & posts to catalog
- Create file(s) with download links
- Provide file information (location, purpose, contents)
- Immediately update GITHUB_FILE_CATALOG.md with ‚è≥ PENDING status

**Step 2**: AI constructs & posts URLs
- Construct raw GitHub URL(s): `https://raw.githubusercontent.com/cbenson85/GEM_Trading_System/refs/heads/main/[FILE_PATH]`
- Post URLs in plain text for easy copying
- Say: "Copy these URLs and paste them back after upload:"

**Step 3**: User uploads & pastes URLs back
- User uploads file(s) to GitHub
- User copies the URL(s) from chat
- User pastes them back in chat

**Step 4**: AI verifies files
- AI uses web_fetch on each pasted URL
- AI confirms file contents
- AI updates GITHUB_FILE_CATALOG.md with ‚úÖ VERIFIED status
- AI updates CURRENT_CATCHUP_PROMPT.md if needed

### **Critical Rules:**
- ‚ùå **AI CANNOT** fetch raw GitHub URLs directly without user providing them first
- ‚úÖ **AI CAN** fetch URLs that user pastes back
- üîÑ **EVERY file** created must follow this verification workflow
- üìù **EVERY verification** updates the catalog and this prompt
- üö´ **NO EXCEPTIONS** - If file isn't verified, it doesn't exist to AI

**Full Documentation:** [FILE_VERIFICATION_PROTOCOL.md](https://raw.githubusercontent.com/cbenson85/GEM_Trading_System/refs/heads/main/FILE_VERIFICATION_PROTOCOL.md)

---

## üíæ PROGRESS LOG

**2025-11-01 02:09** - Phase 1 Complete
- Action: Completed audit of existing system
- Result: Identified fabricated backtest data, cleared portfolio, established new methodology

**2025-11-01** - Phase 2 Started
- Action: Building catch-up prompt system and data infrastructure
- Result: System architecture defined

**2025-11-01** - 10-Year Scan Complete (2014-2024)
- Action: Ran full explosive stock scan
- Result: 244 explosive stocks found, filtered to 170 clean + 69 COVID-era

**2025-11-02 03:15** - File Verification Protocol Complete
- Action: Established and documented file verification workflow
- Result: Zero broken links, perfect file continuity

**2025-11-02 03:15** - Data Backup System Fixed
- Action: Updated filter script with merge logic
- Result: Data accumulates safely, no loss on subsequent scans

**2025-11-02 04:38** - 2013 Scan Complete
- Action: Scanned 2013 for explosive stocks
- Result: Found 199 stocks (including 12 from 2013), merged to 189 clean + 81 COVID

**2025-11-02 16:21** - 2010-2012 Scan Complete
- Action: Scanned 2010-2012 for explosive stocks
- Result: Found 11 new stocks, merged to **200 clean** + 81 COVID

**2025-11-02 16:21** - GitHub Actions Workflow Fixed
- Action: Updated workflow to include filter step and commit all files
- Result: Full automation achieved - no manual steps required

**2025-11-02 17:00** - Pre-Catalyst Analysis Framework Created
- Action: Created comprehensive framework (12 categories, 100+ data points)
- Result: Complete methodology for analyzing 180 days before explosions

**2025-11-02 17:00** - Phase 2 Complete ‚úÖ
- Action: All infrastructure verified and working
- Result: Ready to begin Phase 3 data collection

**2025-11-02 17:30** - Standard Data Format Created
- Action: Created standardized template for all stock data
- Result: Ready for data enrichment and collection

**2025-11-02 18:00** - Sustainability Filter V1.0 Failed
- Action: Ran initial sustainability filter (90% retention, 30 days)
- Result: All 200 stocks failed due to incomplete data and wrong criteria

**2025-11-02 18:15** - CLEAN.json Restored
- Action: Restored 200 stocks from backup after filter emptied file
- Result: All data recovered, no loss

**2025-11-02 18:30** - Sustainability Filter V2.0 Created
- Action: Rebuilt filter with correct criteria (80% retention, 21 days)
- Result: Filter uses Polygon API to find actual catalyst windows

**2025-11-02 18:40** - Developer Tier Optimization
- Action: Discovered user has Polygon Developer tier (unlimited API)
- Result: Removed rate limiting, optimized for 5-15 minute runtime

**2025-11-02 18:45** - Sustainability Filter V2.0 Running
- Action: Started workflow to test all 200 stocks
- Result: **RUNNING NOW** - Expected completion in 5-15 minutes

---

## ‚ö†Ô∏è CRITICAL REMINDERS

1. **NO FABRICATION**: All data must be verified. If unsure, say so immediately.
2. **FALSE MISS PRINCIPLE**: When backtesting, check discarded stocks for explosive growth
3. **USER DOES COPY/PASTE ONLY**: All automation, no manual data entry for user
4. **STORE EVERYTHING**: All backtest data, decisions, refinements must be saved
5. **10-YEAR VALIDATION**: System must work consistently across 10 years before live use
6. **FILE VERIFICATION**: Every file must be verified using the protocol above
7. **DATA PRESERVATION**: Filter uses merge logic - existing data is never lost
8. **FULL AUTOMATION**: Workflow handles everything - user just triggers and monitors
9. **SUSTAINABILITY FIRST**: Don't waste weeks analyzing pump & dumps
10. **DEVELOPER TIER**: Use unlimited API access for fast, efficient results

---

## üöÄ HOW TO CONTINUE

1. Read this entire prompt
2. **WAIT for sustainability filter to complete** (should be done already or very soon)
3. Analyze sustainability results (summary.json, CLEAN.json, UNSUSTAINABLE.json)
4. Document findings and update this prompt
5. Proceed with pre-catalyst data collection for sustainable stocks ONLY
6. Update this prompt after each major milestone
7. **VERIFY FILES**: Follow verification protocol for any new files

---

## üìä SUSTAINABILITY FILTER STATUS

**Current Status**: ‚è≥ RUNNING (started ~2025-11-02 18:45)

**What's Happening:**
- Testing all 200 stocks from CLEAN.json
- Finding actual catalyst windows using Polygon API
- Calculating 21-day retention rates
- Filtering sustainable vs unsustainable stocks

**Expected Output:**
- **CLEAN.json**: ~60-120 sustainable stocks (tradeable)
- **UNSUSTAINABLE.json**: ~80-140 pump & dumps (archived)
- **sustainability_summary.json**: Detailed statistics

**Expected Completion**: Within 15 minutes of start time

**Next Action**: Verify results, analyze findings, proceed with Phase 3

---

## üéØ PHASE 3 READINESS

**What We Have:**
- ‚úÖ 200 explosive stocks identified (2010-2024, excluding COVID)
- ‚è≥ Sustainability filter running (will produce ~60-120 tradeable stocks)
- ‚úÖ Complete data infrastructure
- ‚úÖ File verification system working
- ‚úÖ Data backup system working (merge logic)
- ‚úÖ GitHub Actions fully automated
- ‚úÖ All tools and scripts verified
- ‚úÖ Comprehensive pre-catalyst analysis framework (12 categories)
- ‚úÖ Standard data format template
- ‚úÖ Phase 3 implementation plan
- ‚úÖ Polygon Developer tier (unlimited API access)

**What We Need to Do (After Filter):**
- ‚è≥ Analyze sustainability results
- ‚ùå Build data collection scripts for sustainable stocks
- ‚ùå Collect 180-day pre-catalyst data for sustainable stocks
- ‚ùå Build pattern scanning/correlation tools
- ‚ùå Analyze patterns and extract screening criteria
- ‚ùå Document all discoveries

**Estimated Time for Phase 3:**
- Sustainability analysis: 1 hour
- Data collection script development: 3-5 days
- Full dataset collection: 1-2 weeks (with Developer tier speed)
- Pattern analysis: 1-2 weeks
- **Total: 4-5 weeks of systematic work**

---

**END OF CATCH-UP PROMPT**
Generated by: GEM Catch-Up Prompt System v2.2 (with Sustainability Filter V2.0)
Last Updated: 2025-11-02 18:45:00
System Status: Phase 3 In Progress - Sustainability Filter Running
Next Milestone: Analyze filter results and begin pre-catalyst data collection
