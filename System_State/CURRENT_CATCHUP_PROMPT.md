# GEM Trading System - AI Catch-Up Prompt
**Last Updated**: 2025-11-02 17:30:00
**System Version**: 5.0.1-REBUILD
**Current Phase**: Phase 3: Stock Discovery & Pre-Catalyst Analysis - STARTING

---

## üö® CRITICAL REMINDERS FOR AI - READ THIS FIRST

### 1. FILE CATALOG MUST BE UPDATED FOR EVERY FILE CREATION - NO EXCEPTIONS!

When AI creates ANY file:
1. ‚úÖ Create file with download link
2. ‚úÖ **IMMEDIATELY UPDATE GITHUB_FILE_CATALOG.md with ‚è≥ PENDING status**
3. ‚úÖ Post GitHub URL for user to copy
4. ‚úÖ Wait for user to upload and paste back
5. ‚úÖ Verify file via web_fetch
6. ‚úÖ **UPDATE GITHUB_FILE_CATALOG.md with ‚úÖ VERIFIED status**
7. ‚úÖ Update CURRENT_CATCHUP_PROMPT.md if major milestone

**This is MANDATORY. Not optional. Not negotiable. ALWAYS.**

See [FILE_VERIFICATION_PROTOCOL.md](https://raw.githubusercontent.com/cbenson85/GEM_Trading_System/refs/heads/main/FILE_VERIFICATION_PROTOCOL.md) for complete workflow.

### 2. CATCH-UP PROMPT + SYSTEM_STATE.JSON ARE PAIRED FILES - UPDATE BOTH!

**When CURRENT_CATCHUP_PROMPT.md is updated, system_state.json MUST be updated too!**

These files work together:
- **CURRENT_CATCHUP_PROMPT.md** = Human-readable narrative
- **system_state.json** = Machine-readable structured data

They must ALWAYS have:
- ‚úÖ Matching timestamps
- ‚úÖ Matching current phase/status
- ‚úÖ Matching next steps
- ‚úÖ Same key information

**Never update one without the other!**

---

## üéØ CRITICAL CONTEXT

### What You're Walking Into
System rebuilt from ground up after previous model fabricated backtest data. All infrastructure now complete and verified. Pre-catalyst analysis framework created. Ready to begin collecting 180-day pre-catalyst data for 200 explosive stocks.

### Immediate Priority
Create Standard Data Format Template, then begin systematic data collection for all 200 stocks

---

## üìä SYSTEM STATUS

### Current State
- **Portfolio**: CLEARED - Starting fresh
- **Cash Available**: $10,000 (reset)
- **System Stage**: Phase 3 Ready - Data Collection Beginning
- **Last Completed Phase**: Phase 2: System Requirements & Data Infrastructure - COMPLETE ‚úÖ
- **Next Phase**: Phase 3: Stock Discovery & Pre-Catalyst Analysis

### Data Verification Status

- Current Prices: ‚úÖ VERIFIED (Polygon API)
- Volume Data: ‚úÖ VERIFIED (Polygon API)
- Float Data: ‚ö†Ô∏è PARTIAL (Polygon API - not all stocks)
- Explosive Stock Catalog: ‚úÖ VERIFIED (200 stocks, expanded to 2010-2024)
- Backtest Infrastructure: ‚úÖ COMPLETE
- Data Backup System: ‚úÖ FIXED (merge logic implemented)
- Pre-Catalyst Framework: ‚úÖ CREATED & VERIFIED
- Historical Analysis: ‚è≥ STARTING (Phase 3)

---

## üóÇÔ∏è FILE STRUCTURE & LOCATIONS

**Repository**: https://github.com/cbenson85/GEM_Trading_System

### Core System Files (VERIFIED)

- `/Current_System/GEM_v5_Master_Screening_Protocol.md` - Core screening rules
- `/Current_System/Trading_Rules_Complete.md` - Trading operations guide
- `/FILE_VERIFICATION_PROTOCOL.md` - File access methodology (MANDATORY)
- `/COVID_ERA_EXCLUSION_RULE.md` - Critical COVID-era filter rule

### Data Files (VERIFIED - Location & Status)

- `/Verified_Backtest_Data/explosive_stocks_catalog.json` - ‚úÖ Latest scan results (11 stocks from 2010-2012)
- `/Verified_Backtest_Data/explosive_stocks_CLEAN.json` - ‚úÖ **200 clean stocks** (2010-2019, 2022-2024)
- `/Verified_Backtest_Data/explosive_stocks_COVID_ERA.json` - ‚úÖ 81 COVID-era stocks (archived)
- `/Verified_Backtest_Data/refinement_history.json` - ‚úÖ System evolution log
- `/Verified_Backtest_Data/README.md` - ‚úÖ Data quality standards
- `/Verified_Backtest_Data/filter_summary.json` - ‚úÖ CREATED (tracks merge statistics)
- `/Verified_Backtest_Data/PRE_CATALYST_ANALYSIS_FRAMEWORK.md` - ‚úÖ VERIFIED (comprehensive framework)
- `/Verified_Backtest_Data/PHASE_3_IMPLEMENTATION_PLAN.md` - ‚è≥ PENDING VERIFICATION (uploaded but not verified yet)
- `/Verified_Backtest_Data/pre_catalyst_analysis/` - ‚ùå TO BE CREATED (Phase 3 data)
- `/Verified_Backtest_Data/backtest_runs/` - ‚ùå TO BE CREATED (Phase 4+)

### Active Scripts (VERIFIED)

- `/explosive_stock_scanner.py` - ‚úÖ Multi-year scanner (GitHub Actions)
- `/filter_covid_era.py` - ‚úÖ COVID filter WITH MERGE LOGIC (updated 2025-11-02)
- `/.github/workflows/explosive_stock_scan_workflow.yml` - ‚úÖ FIXED (includes filter step, commits all files)

### Archived Content (DO NOT USE FOR DECISIONS)

- `/Archive_Unverified/` - ‚úÖ Contains old unverified data (reference only)
- `/Backtest_Results/` - ‚ö†Ô∏è UNVERIFIED (framework reference only)
- `/Strategy_Evolution/` - ‚ö†Ô∏è UNVERIFIED (concepts only, not data)

### üìÇ Complete GitHub File Catalog

**Full catalog with all links**: [GITHUB_FILE_CATALOG.md](https://raw.githubusercontent.com/cbenson85/GEM_Trading_System/refs/heads/main/GITHUB_FILE_CATALOG.md)

**Critical files for AI:**
- [CURRENT_CATCHUP_PROMPT.md](https://raw.githubusercontent.com/cbenson85/GEM_Trading_System/refs/heads/main/System_State/CURRENT_CATCHUP_PROMPT.md) - This file
- [system_state.json](https://raw.githubusercontent.com/cbenson85/GEM_Trading_System/refs/heads/main/System_State/system_state.json) - System data
- [FILE_VERIFICATION_PROTOCOL.md](https://raw.githubusercontent.com/cbenson85/GEM_Trading_System/refs/heads/main/FILE_VERIFICATION_PROTOCOL.md) - File access method

---

## üî¨ BACKTESTING METHODOLOGY

### Approach

1. ‚úÖ Find stocks with 500%+ gains in any 6-month window (2010-2024) - COMPLETE
2. ‚è≥ Deep dive into 180 days PRE-CATALYST for each stock - Phase 3 STARTING
3. ‚ùå Analyze: price, volume, sentiment, leadership, news, patterns - Phase 3
4. ‚ùå Identify correlations between explosive stocks - Phase 3
5. ‚ùå Build screener based on correlations - Phase 3+
6. ‚ùå Backtest on random historical dates - Phase 4+
7. ‚ùå Apply FALSE MISS principle to discarded stocks - Phase 4+
8. ‚ùå Track picks AND discards with full performance data - Phase 4+
9. ‚ùå Refine until consistent 10-year performance - Phase 4+
10. ‚úÖ Store ALL data for future refinement - Infrastructure ready

### Current Progress

**Phase 1**: Discovery & Documentation Audit - ‚úÖ COMPLETE
**Phase 2**: System Requirements & Data Infrastructure - ‚úÖ COMPLETE
- ‚úÖ Multi-year explosive stock scan complete (200 stocks from 2010-2024)
- ‚úÖ COVID-era filter applied (200 clean, 81 archived)
- ‚úÖ File verification protocol established
- ‚úÖ Data backup system fixed (merge logic)
- ‚úÖ All infrastructure verified and working
- ‚úÖ Pre-catalyst analysis framework created (12 categories, 100+ data points)
- ‚úÖ GitHub Actions workflow fixed (full automation, no manual steps)

**Phase 3**: Stock Discovery & Pre-Catalyst Analysis - ‚è≥ STARTING NOW
- ‚è≥ Create Standard Data Format Template (NEXT IMMEDIATE STEP)
- ‚ùå Build data collection scripts (price, volume, news, SEC, insider, etc.)
- ‚ùå Enrich existing 200 stocks with standard format
- ‚ùå Deep-dive 180-day pre-catalyst analysis
- ‚ùå Pattern discovery across all stocks
- ‚ùå Correlation matrix building
- ‚ùå Screener criteria extraction

### Data Sources

- Price/Volume: Polygon API (Developer tier) + Yahoo Finance backup
- News/Sentiment: Web scraping (Google, Yahoo Finance)
- Insider Trading: Free sources (Finviz, OpenInsider, SEC Form 4)
- SEC Filings: SEC EDGAR (free)
- Float/Shares: Polygon API + manual verification

---

## üìà DISCOVERED PATTERNS

### Verified Correlations
NONE YET - Will be discovered during Phase 3 analysis

### Current Scoring Criteria
CURRENT v5.0.1 CRITERIA - Will be updated based on Phase 3 pattern discovery

### Refinement History
3 refinements logged:
1. System reset (2025-11-01) - Cleared unverified data
2. COVID-era exclusion rule (2025-11-01) - Exclude 2020-2021 from patterns
3. Expanded historical coverage (2025-11-02) - Added 2010-2012 data (11 stocks)

---

## üéØ DECISIONS MADE

### Key Decisions
1. ‚úÖ Mark all previous backtest results as UNVERIFIED
2. ‚úÖ Clear all current portfolio positions - starting fresh
3. ‚úÖ Build from verified data only - no fabrication
4. ‚úÖ Use 500%+ in 6 months as explosive stock criteria
5. ‚úÖ Analyze 180 days pre-catalyst for pattern discovery
6. ‚úÖ Store all backtest data (picks AND discards) for refinement
7. ‚úÖ Apply false miss principle in all backtests
8. ‚úÖ User only does copy/paste - full automation required
9. ‚úÖ Free data sources only (no paid APIs except Polygon)
10. ‚úÖ Exclude COVID-era (2020-2021) from pattern analysis
11. ‚úÖ Implement merge logic in filter to prevent data loss
12. ‚úÖ Fix GitHub Actions workflow for full automation (no manual filter runs)
13. ‚úÖ Expand historical coverage back to 2010 for more data
14. ‚úÖ Create comprehensive pre-catalyst analysis framework (12 categories)

### Rules Established
1. **NO FABRICATION** - Verify or say you can't
2. **FALSE MISS CHECK** - Always check discarded stocks for explosive growth
3. **STORE EVERYTHING** - All decisions, data, refinements must be saved
4. **COPY/PASTE ONLY** - User should never manually enter data
5. **10-YEAR VALIDATION** - System must prove consistency before live trading
6. **FILE VERIFICATION** - Every file must be verified using the established protocol
7. **DATA PRESERVATION** - Filter script uses merge logic (no data loss on subsequent scans)
8. **FULL AUTOMATION** - Workflow handles scanning, filtering, and committing without manual steps

---

## üìù WHAT NEEDS TO HAPPEN NEXT

### Phase 3 Tasks (READY TO BEGIN)

**Immediate Next Step:**
1. ‚è≥ Create Standard Data Format Template (defines structure for all 200 stocks)

**After Template Approved:**
2. Build data enrichment script (standardize existing CLEAN.json to new format)
3. Build data collection scripts:
   - Price/volume collector (Polygon API)
   - SEC filings collector (EDGAR)
   - Insider trading collector (Form 4, OpenInsider)
   - News/sentiment collector (web scraping)
   - Float/ownership collector (Polygon + Finviz)
   - Financial metrics collector (Polygon + SEC)
4. Collect 180-day pre-catalyst data for all 200 stocks
5. Build pattern scanning/correlation tools
6. Analyze patterns and extract screening criteria
7. Document all findings

### Blockers/Questions
NONE - Ready to create standard format template

---

## üîó IMPORTANT LINKS

- **GitHub Repo**: https://github.com/cbenson85/GEM_Trading_System
- **Polygon API**: Developer tier (key: pvv6DNmKAoxojCc0B5HOaji6I_k1egv0)
- **Data Storage**: /Verified_Backtest_Data/ (active and verified)
- **File Catalog**: [GITHUB_FILE_CATALOG.md](https://raw.githubusercontent.com/cbenson85/GEM_Trading_System/refs/heads/main/GITHUB_FILE_CATALOG.md)

---

## üîê FILE VERIFICATION PROTOCOL (MANDATORY)

### **The Problem We Solved:**
AI was creating files without verifying it could read them back from GitHub. This created broken links and lost context between sessions.

### **The Solution - REQUIRED WORKFLOW:**

**When AI Creates New File(s):**

**Step 1**: AI provides files & posts to catalog
- Create file(s) with download links
- Provide file information (location, purpose, contents)
- Immediately update GITHUB_FILE_CATALOG.md with ‚è≥ PENDING status

**Step 2**: AI constructs & posts URLs
- Construct raw GitHub URL(s): `https://raw.githubusercontent.com/cbenson85/GEM_Trading_System/refs/heads/main/[FILE_PATH]`
- Post URLs in plain text for easy copying
- Say: "Copy these URLs and paste them back after upload:"

**Step 3**: User uploads & pastes URLs back
- User uploads file(s) to GitHub
- User copies the URL(s) from chat
- User pastes them back in chat

**Step 4**: AI verifies files
- AI uses web_fetch on each pasted URL
- AI confirms file contents
- AI updates GITHUB_FILE_CATALOG.md with ‚úÖ VERIFIED status
- AI updates CURRENT_CATCHUP_PROMPT.md if needed

### **Critical Rules:**
- ‚ùå **AI CANNOT** fetch raw GitHub URLs directly without user providing them first
- ‚úÖ **AI CAN** fetch URLs that user pastes back
- üîÑ **EVERY file** created must follow this verification workflow
- üìù **EVERY verification** updates the catalog and this prompt
- üö´ **NO EXCEPTIONS** - If file isn't verified, it doesn't exist to AI

**Full Documentation:** [FILE_VERIFICATION_PROTOCOL.md](https://raw.githubusercontent.com/cbenson85/GEM_Trading_System/refs/heads/main/FILE_VERIFICATION_PROTOCOL.md)

---

## üîÑ DATA BACKUP SYSTEM (FIXED 2025-11-02)

### **The Problem:**
Original filter script OVERWROTE CLEAN.json and COVID.json files on each run, causing data loss when incremental scans ran.

### **The Solution:**
Updated `filter_covid_era.py` with **MERGE LOGIC**:

**How It Works:**
1. Filter loads existing CLEAN.json and COVID.json (if they exist)
2. Filter reads new stocks from catalog.json
3. Filter merges new stocks (checks for ticker+year duplicates)
4. Filter saves updated cumulative files

**Result:**
- ‚úÖ No data loss on subsequent scans
- ‚úÖ No duplicates (ticker+year key)
- ‚úÖ Accumulates findings over time
- ‚úÖ catalog.json remains "latest scan snapshot"

**Example:**
```
Scan 1 (2014-2024): catalog.json (244 stocks)
‚Üí Filter: CLEAN.json (170) + COVID.json (69)

Scan 2 (2013): catalog.json (199 stocks)
‚Üí Filter: Loads existing 170, adds 19 new = 189 total

Scan 3 (2010-2012): catalog.json (11 stocks)
‚Üí Filter: Loads existing 189, adds 11 new = 200 total

No data loss! ‚úÖ
```

---

## üîß GITHUB ACTIONS AUTOMATION (FIXED 2025-11-02)

### **The Problem:**
Workflow was missing the filter step, so user had to manually run `filter_covid_era.py` and push files locally after each scan.

### **The Solution:**
Updated `explosive_stock_scan_workflow.yml` to include:

**New Steps Added:**
1. Run COVID-era filter automatically after scan
2. Verify filter results (show merge statistics)
3. Commit ALL 4 files (catalog.json, CLEAN.json, COVID_ERA.json, filter_summary.json)
4. Push everything to GitHub

**Result:**
- ‚úÖ Full automation - no manual steps needed
- ‚úÖ All files committed and pushed automatically
- ‚úÖ Merge statistics visible in GitHub Actions logs
- ‚úÖ User can trigger scan and walk away

---

## üíæ PROGRESS LOG

**2025-11-01 02:09** - Phase 1 Complete
- Action: Completed audit of existing system
- Result: Identified fabricated backtest data, cleared portfolio, established new methodology

**2025-11-01** - Phase 2 Started
- Action: Building catch-up prompt system and data infrastructure
- Result: System architecture defined

**2025-11-01** - 10-Year Scan Complete (2014-2024)
- Action: Ran full explosive stock scan
- Result: 244 explosive stocks found, filtered to 170 clean + 69 COVID-era

**2025-11-02 03:15** - File Verification Protocol Complete
- Action: Established and documented file verification workflow
- Result: Zero broken links, perfect file continuity

**2025-11-02 03:15** - Data Backup System Fixed
- Action: Updated filter script with merge logic
- Result: Data accumulates safely, no loss on subsequent scans

**2025-11-02 04:38** - 2013 Scan Complete
- Action: Scanned 2013 for explosive stocks
- Result: Found 199 stocks (including 12 from 2013), merged to 189 clean + 81 COVID

**2025-11-02 16:21** - 2010-2012 Scan Complete
- Action: Scanned 2010-2012 for explosive stocks
- Result: Found 11 new stocks, merged to **200 clean** + 81 COVID

**2025-11-02 16:21** - GitHub Actions Workflow Fixed
- Action: Updated workflow to include filter step and commit all files
- Result: Full automation achieved - no manual steps required

**2025-11-02 17:00** - Pre-Catalyst Analysis Framework Created
- Action: Created comprehensive framework (12 categories, 100+ data points)
- Result: Complete methodology for analyzing 180 days before explosions

**2025-11-02 17:00** - Phase 2 Complete ‚úÖ
- Action: All infrastructure verified and working
- Result: Ready to begin Phase 3 data collection

---

## ‚ö†Ô∏è CRITICAL REMINDERS

1. **NO FABRICATION**: All data must be verified. If unsure, say so immediately.
2. **FALSE MISS PRINCIPLE**: When backtesting, check discarded stocks for explosive growth
3. **USER DOES COPY/PASTE ONLY**: All automation, no manual data entry for user
4. **STORE EVERYTHING**: All backtest data, decisions, refinements must be saved
5. **10-YEAR VALIDATION**: System must work consistently across 10 years before live use
6. **FILE VERIFICATION**: Every file must be verified using the protocol above
7. **DATA PRESERVATION**: Filter uses merge logic - existing data is never lost
8. **FULL AUTOMATION**: Workflow handles everything - user just triggers and monitors

---

## üöÄ HOW TO CONTINUE

1. Read this entire prompt
2. Ask clarifying questions if anything is unclear
3. Confirm you understand current state and next phase
4. Wait for user approval before proceeding with template creation
5. Update this prompt after each major milestone
6. **VERIFY FILES**: Follow verification protocol for any new files

---

## üìä READY FOR PHASE 3

**What We Have:**
- ‚úÖ **200 clean explosive stocks** (2010-2019, 2022-2024) - expanded dataset!
- ‚úÖ 81 COVID-era stocks archived (2020-2021)
- ‚úÖ Complete data infrastructure
- ‚úÖ File verification system working
- ‚úÖ Data backup system working (merge logic)
- ‚úÖ GitHub Actions fully automated (no manual steps)
- ‚úÖ All tools and scripts verified
- ‚úÖ Comprehensive pre-catalyst analysis framework (12 categories, 100+ data points)
- ‚úÖ Phase 3 implementation plan created

**What We Need to Do:**
- ‚è≥ **Create Standard Data Format Template** (NEXT IMMEDIATE STEP)
- ‚ùå Build data enrichment script (standardize existing 200 stocks)
- ‚ùå Build data collection scripts (8 collectors for different data types)
- ‚ùå Collect 180-day pre-catalyst data for all 200 stocks
- ‚ùå Build pattern scanning/correlation tools
- ‚ùå Analyze patterns and extract screening criteria
- ‚ùå Document all discoveries

**Estimated Time for Phase 3:**
- Template creation: 1-2 hours
- Data enrichment: 1-2 days
- Collection script development: 1 week
- Full dataset collection: 2-3 weeks (API rate limits)
- Pattern analysis: 1-2 weeks
- **Total: 6-8 weeks of systematic work**

---

## üìã DATASET NOTES

### Current CLEAN.json Structure (Inconsistent)
The 200 stocks in CLEAN.json have **TWO different formats**:

**Old Format** (most 2014-2024 stocks):
```json
{
  "ticker": "ABVC",
  "year": 2014,
  "gain_percent": 900,
  "days_to_peak": 28
}
```

**New Format** (2010-2013 stocks, some others):
```json
{
  "ticker": "ABUS",
  "year_discovered": 2013,
  "catalyst_date": "2014-03-10",
  "entry_price": 4.8,
  "peak_price": 30.75,
  "gain_percent": 540.6,
  "days_to_peak": 171,
  "entry_date": "2013-07-03",
  "sector": "Unknown",
  "verified": true,
  "data_source": "Yahoo Finance",
  "all_explosive_windows": 35,
  "covid_era": false
}
```

**Action Required**: Standardize all 200 stocks to unified format before pre-catalyst data collection

### Notable Stocks:
- **ABVC**: Appears in 9 different years (2010-2015, 2018-2019, 2024) with extreme gains
  - 2011: 5,328% gain
  - 2015: 416,567% gain (most extreme ever)
- **ASTH**: Appears in 6 years with strong gains
  - 2013: 3,129% gain
- Dataset spans **15 years** (2010-2024, excluding 2020-2021)

---

**END OF CATCH-UP PROMPT**
Generated by: GEM Catch-Up Prompt System v2.1 (with Phase 3 readiness)
Last Updated: 2025-11-02 17:00:00
System Status: Phase 2 Complete - Phase 3 Starting (Template Creation Next)
