name: Optimized Multi-Year Scan (Skip COVID)

on:
  workflow_dispatch:
    inputs:
      confirmation:
        description: 'Type "RUN" to start 4 parallel scans (2010-2019, 2022-2024)'
        required: true
        default: ''

jobs:
  scan_2010_2012:
    name: Scan 2010-2012
    runs-on: ubuntu-latest
    if: ${{ github.event.inputs.confirmation == 'RUN' }}
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
      
      - name: Install dependencies
        run: pip install requests yfinance
      
      - name: Create output directory
        run: mkdir -p Verified_Backtest_Data
      
      - name: Run scan
        env:
          POLYGON_API_KEY: ${{ secrets.POLYGON_API_KEY }}
        run: |
          echo "üîç Scanning 2010-2012 (3 years)"
          python explosive_stock_scanner.py --start-year 2010 --end-year 2012
      
      - name: Upload results
        uses: actions/upload-artifact@v4
        with:
          name: catalog-2010-2012
          path: Verified_Backtest_Data/explosive_stocks_catalog.json

  scan_2013_2015:
    name: Scan 2013-2015
    runs-on: ubuntu-latest
    if: ${{ github.event.inputs.confirmation == 'RUN' }}
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
      
      - name: Install dependencies
        run: pip install requests yfinance
      
      - name: Create output directory
        run: mkdir -p Verified_Backtest_Data
      
      - name: Run scan
        env:
          POLYGON_API_KEY: ${{ secrets.POLYGON_API_KEY }}
        run: |
          echo "üîç Scanning 2013-2015 (3 years)"
          python explosive_stock_scanner.py --start-year 2013 --end-year 2015
      
      - name: Upload results
        uses: actions/upload-artifact@v4
        with:
          name: catalog-2013-2015
          path: Verified_Backtest_Data/explosive_stocks_catalog.json

  scan_2016_2019:
    name: Scan 2016-2019
    runs-on: ubuntu-latest
    if: ${{ github.event.inputs.confirmation == 'RUN' }}
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
      
      - name: Install dependencies
        run: pip install requests yfinance
      
      - name: Create output directory
        run: mkdir -p Verified_Backtest_Data
      
      - name: Run scan
        env:
          POLYGON_API_KEY: ${{ secrets.POLYGON_API_KEY }}
        run: |
          echo "üîç Scanning 2016-2019 (4 years)"
          python explosive_stock_scanner.py --start-year 2016 --end-year 2019
      
      - name: Upload results
        uses: actions/upload-artifact@v4
        with:
          name: catalog-2016-2019
          path: Verified_Backtest_Data/explosive_stocks_catalog.json

  scan_2022_2024:
    name: Scan 2022-2024
    runs-on: ubuntu-latest
    if: ${{ github.event.inputs.confirmation == 'RUN' }}
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
      
      - name: Install dependencies
        run: pip install requests yfinance
      
      - name: Create output directory
        run: mkdir -p Verified_Backtest_Data
      
      - name: Run scan
        env:
          POLYGON_API_KEY: ${{ secrets.POLYGON_API_KEY }}
        run: |
          echo "üîç Scanning 2022-2024 (3 years)"
          python explosive_stock_scanner.py --start-year 2022 --end-year 2024
      
      - name: Upload results
        uses: actions/upload-artifact@v4
        with:
          name: catalog-2022-2024
          path: Verified_Backtest_Data/explosive_stocks_catalog.json

  merge_and_filter:
    name: Merge + Sustainability Filter
    runs-on: ubuntu-latest
    needs: [scan_2010_2012, scan_2013_2015, scan_2016_2019, scan_2022_2024]
    if: ${{ always() && github.event.inputs.confirmation == 'RUN' }}
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
      
      - name: Create directories
        run: mkdir -p Verified_Backtest_Data
      
      - name: Download all catalogs
        uses: actions/download-artifact@v4
        with:
          pattern: catalog-*
          path: temp/
          merge-multiple: false
      
      - name: List downloaded files
        run: |
          echo "Downloaded catalogs:"
          find temp -name "*.json" -type f
      
      - name: Merge all catalogs
        run: |
          python3 << 'EOF'
          import json
          import glob
          from datetime import datetime
          
          print("\n" + "="*60)
          print("MERGING CATALOGS (NO COVID YEARS)")
          print("="*60 + "\n")
          
          # Find all catalog files
          catalog_files = sorted(glob.glob('temp/**/explosive_stocks_catalog.json', recursive=True))
          
          print(f"Found {len(catalog_files)} catalog files:")
          for f in catalog_files:
              print(f"  - {f}")
          
          if len(catalog_files) == 0:
              print("\n‚ùå No catalog files found!")
              exit(1)
          
          # Load all catalogs
          all_stocks = []
          total_scanned = 0
          total_errors = 0
          total_api_calls = 0
          
          for catalog_file in catalog_files:
              with open(catalog_file, 'r') as f:
                  data = json.load(f)
                  stocks = data.get('stocks', [])
                  scan_info = data.get('scan_info', {})
                  
                  print(f"\n‚úÖ Loaded: {catalog_file}")
                  print(f"   Stocks: {len(stocks)}")
                  print(f"   Period: {scan_info.get('scan_period_start')} to {scan_info.get('scan_period_end')}")
                  
                  all_stocks.extend(stocks)
                  total_scanned += scan_info.get('total_stocks_scanned', 0)
                  total_errors += scan_info.get('data_errors', 0)
                  total_api_calls += scan_info.get('api_calls_made', 0)
          
          # Remove duplicates
          seen = set()
          unique_stocks = []
          for stock in all_stocks:
              key = f"{stock['ticker']}_{stock['year_discovered']}"
              if key not in seen:
                  unique_stocks.append(stock)
                  seen.add(key)
          
          # Sort by gain percent
          unique_stocks.sort(key=lambda x: x.get('gain_percent', 0), reverse=True)
          
          print(f"\nüìä Merge Summary:")
          print(f"   Total stocks: {len(all_stocks)}")
          print(f"   Unique stocks: {len(unique_stocks)}")
          print(f"   Duplicates removed: {len(all_stocks) - len(unique_stocks)}")
          print(f"   Tickers scanned: {total_scanned:,}")
          print(f"   Years covered: 2010-2019, 2022-2024 (COVID excluded)")
          
          # Create merged catalog
          merged_catalog = {
              'scan_info': {
                  'scan_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                  'scan_period_start': '2010-01-01',
                  'scan_period_end': '2024-12-31',
                  'criteria': '500%+ gain in any 120-day window',
                  'data_sources': ['Polygon API', 'Yahoo Finance'],
                  'coverage': 'Complete - all active US stock tickers (COVID years excluded)',
                  'covid_years_excluded': '2020-2021',
                  'total_stocks_scanned': total_scanned,
                  'total_explosive_stocks_found': len(unique_stocks),
                  'data_errors': total_errors,
                  'api_calls_made': total_api_calls,
                  'scan_complete': True,
                  'merged_from_scans': len(catalog_files)
              },
              'stocks': unique_stocks,
              'metadata': {
                  'last_updated': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                  'verified_by': 'Merged from 4 parallel GitHub Actions scans',
                  'quality_checks_passed': True,
                  'notes': 'COVID years (2020-2021) excluded from scan'
              }
          }
          
          # Save as CLEAN.json (since we already excluded COVID)
          with open('Verified_Backtest_Data/explosive_stocks_CLEAN.json', 'w') as f:
              json.dump(merged_catalog, f, indent=2)
          
          print(f"\n‚úÖ Merged catalog saved as CLEAN.json (COVID already excluded)")
          print(f"\nüöÄ Top 10 Explosive Stocks:")
          for i, stock in enumerate(unique_stocks[:10], 1):
              print(f"   {i:2d}. {stock['ticker']:6s}: {stock['gain_percent']:>7.0f}% in {stock['days_to_peak']:>3d} days ({stock['year_discovered']})")
          EOF
      
      - name: Run Sustainability Filter
        run: |
          echo ""
          echo "üîç Running sustainability filter..."
          python filter_sustainability.py
      
      - name: Display final results
        run: |
          python3 << 'EOF'
          import json
          import os
          
          print("\n" + "="*60)
          print("üìä FINAL RESULTS (2010-2019, 2022-2024)")
          print("="*60 + "\n")
          
          # Before sustainability filter
          with open('Verified_Backtest_Data/explosive_stocks_CLEAN.json', 'r') as f:
              data = json.load(f)
              info = data['scan_info']
              stocks = data.get('stocks', [])
              
              print(f"üìÑ Complete Catalog (COVID excluded):")
              print(f"   Years: 2010-2019, 2022-2024")
              print(f"   Total explosive stocks: {len(stocks):,}")
              print(f"   Tickers scanned: {info.get('total_stocks_scanned', 0):,}")
              print(f"   API calls: {info.get('api_calls_made', 0):,}")
          
          # After sustainability filter
          if os.path.exists('Verified_Backtest_Data/explosive_stocks_UNSUSTAINABLE.json'):
              with open('Verified_Backtest_Data/explosive_stocks_UNSUSTAINABLE.json', 'r') as f:
                  unsustainable = json.load(f)
                  print(f"\nüìÑ After Sustainability Filter:")
                  print(f"   Sustainable stocks (FINAL): {len(stocks) - len(unsustainable.get('stocks', [])):,}")
                  print(f"   Pump-and-dumps removed: {len(unsustainable.get('stocks', [])):,}")
          
          print(f"\n‚úÖ Complete analysis finished!")
          print(f"üì¶ Download 'final-results' artifact for all files")
          EOF
      
      - name: Upload final results
        uses: actions/upload-artifact@v4
        with:
          name: final-results-no-covid
          path: |
            Verified_Backtest_Data/explosive_stocks_CLEAN.json
            Verified_Backtest_Data/explosive_stocks_UNSUSTAINABLE.json
          retention-days: 90
