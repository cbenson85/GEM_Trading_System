name: Phase 3 Parallel Analysis - Complete Fixed

on:
  workflow_dispatch:
    inputs:
      mode:
        description: 'Analysis mode'
        required: true
        default: 'test'
        type: choice
        options:
          - test       # 10 stocks (5 batches Ã— 2 stocks)
          - sample     # 100 stocks (5 batches Ã— 20 stocks)
          - full       # All 694 stocks (10 batches Ã— ~70 stocks)

permissions:
  contents: write

jobs:
  prepare-batches:
    runs-on: ubuntu-latest
    outputs:
      batch_count: ${{ steps.split.outputs.batch_count }}
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Split stocks into batches
      id: split
      env:
        MODE: ${{ github.event.inputs.mode }}
      run: |
        cat > batch_splitter.py << 'SCRIPT'
        import json
        import os
        import sys
        
        # Get mode from environment
        mode = os.environ.get('MODE', 'test')
        
        # Load explosive stocks
        with open('Verified_Backtest_Data/explosive_stocks_CLEAN.json', 'r') as f:
            all_stocks = json.load(f)
        
        # Determine batch parameters
        if mode == "test":
            stocks_to_process = all_stocks[:10]
            batch_count = 5
            print(f"TEST MODE: Processing {len(stocks_to_process)} stocks in {batch_count} batches")
        elif mode == "sample":
            stocks_to_process = all_stocks[:100]
            batch_count = 5
            print(f"SAMPLE MODE: Processing {len(stocks_to_process)} stocks in {batch_count} batches")
        else:  # full
            stocks_to_process = all_stocks
            batch_count = 10
            print(f"FULL MODE: Processing {len(stocks_to_process)} stocks in {batch_count} batches")
        
        # Create batch_inputs directory
        os.makedirs('batch_inputs', exist_ok=True)
        
        # Calculate batch size
        total_stocks = len(stocks_to_process)
        batch_size = total_stocks // batch_count
        remainder = total_stocks % batch_count
        
        # Split into batches
        start_idx = 0
        for i in range(batch_count):
            # Add extra stock to earlier batches if there's a remainder
            current_batch_size = batch_size + (1 if i < remainder else 0)
            end_idx = start_idx + current_batch_size
            
            batch_stocks = stocks_to_process[start_idx:end_idx]
            
            if batch_stocks:
                batch_num = i + 1
                batch_file = f'batch_inputs/batch{batch_num}_stocks.json'
                
                batch_data = {
                    'batch': batch_num,
                    'stocks': batch_stocks
                }
                
                with open(batch_file, 'w') as f:
                    json.dump(batch_data, f, indent=2)
                
                print(f"Created batch {batch_num}: {len(batch_stocks)} stocks")
            
            start_idx = end_idx
        
        # Write output for GitHub Actions
        with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
            f.write(f"batch_count={batch_count}\n")
        
        print(f"Successfully created {batch_count} batches")
        SCRIPT
        
        python batch_splitter.py
        
        echo "ðŸ“ Batch files created:"
        ls -la batch_inputs/
    
    - name: Upload batch files
      uses: actions/upload-artifact@v4
      with:
        name: batch-inputs
        path: batch_inputs/

  analyze-batch-1:
    needs: prepare-batches
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        pip install requests pandas numpy yfinance beautifulsoup4 lxml
    
    - name: Download batch files
      uses: actions/download-artifact@v4
      with:
        name: batch-inputs
        path: batch_inputs/
    
    - name: Run comprehensive analysis
      env:
        POLYGON_API_KEY: ${{ secrets.POLYGON_API_KEY }}
      run: |
        echo "ðŸ”¬ Analyzing Batch 1..."
        
        # Create output directory
        mkdir -p Verified_Backtest_Data
        
        # Try to run the collector
        if [ -f "phase3_comprehensive_collector_FIXED.py" ]; then
          echo "Using comprehensive collector FIXED"
          python phase3_comprehensive_collector_FIXED.py batch1 batch_inputs/batch1_stocks.json
        elif [ -f "phase3_comprehensive_collector.py" ]; then
          echo "Using original collector"
          python phase3_comprehensive_collector.py batch1 batch_inputs/batch1_stocks.json
        else
          echo "No collector found - creating minimal output"
          cat > minimal_collector.py << 'COLLECTOR'
        import json
        import sys
        from datetime import datetime
        
        batch_name = sys.argv[1] if len(sys.argv) > 1 else "batch1"
        batch_file = sys.argv[2] if len(sys.argv) > 2 else "batch_inputs/batch1_stocks.json"
        
        with open(batch_file, 'r') as f:
            batch_data = json.load(f)
        
        results = []
        for stock in batch_data.get('stocks', []):
            results.append({
                'ticker': stock.get('ticker'),
                'gain_percent': stock.get('gain_percent'),
                'analysis_status': 'minimal',
                'total_score_pre': 0
            })
        
        output = {
            'batch_name': batch_name,
            'analysis_date': datetime.now().isoformat(),
            'successful_analyses': len(results),
            'failed_analyses': 0,
            'results': results
        }
        
        with open(f'Verified_Backtest_Data/phase3_batch_{batch_name}_analysis.json', 'w') as f:
            json.dump(output, f, indent=2)
        print(f"Created minimal output with {len(results)} stocks")
        COLLECTOR
          python minimal_collector.py batch1 batch_inputs/batch1_stocks.json
        fi
        
        # Verify output exists
        if [ -f "Verified_Backtest_Data/phase3_batch_batch1_analysis.json" ]; then
          echo "âœ… Output file created successfully"
        else
          echo "âŒ No output file created!"
          echo '{"results": []}' > Verified_Backtest_Data/phase3_batch_batch1_analysis.json
        fi
    
    - name: Upload results
      uses: actions/upload-artifact@v4
      with:
        name: batch1-results
        path: Verified_Backtest_Data/phase3_batch_batch1_analysis.json
      if: always()

  analyze-batch-2:
    needs: prepare-batches
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        pip install requests pandas numpy yfinance beautifulsoup4 lxml
    
    - name: Download batch files
      uses: actions/download-artifact@v4
      with:
        name: batch-inputs
        path: batch_inputs/
    
    - name: Run comprehensive analysis
      env:
        POLYGON_API_KEY: ${{ secrets.POLYGON_API_KEY }}
      run: |
        echo "ðŸ”¬ Analyzing Batch 2..."
        mkdir -p Verified_Backtest_Data
        
        if [ -f "phase3_comprehensive_collector_FIXED.py" ]; then
          python phase3_comprehensive_collector_FIXED.py batch2 batch_inputs/batch2_stocks.json
        elif [ -f "phase3_comprehensive_collector.py" ]; then
          python phase3_comprehensive_collector.py batch2 batch_inputs/batch2_stocks.json
        else
          echo '{"results": []}' > Verified_Backtest_Data/phase3_batch_batch2_analysis.json
        fi
    
    - name: Upload results
      uses: actions/upload-artifact@v4
      with:
        name: batch2-results
        path: Verified_Backtest_Data/phase3_batch_batch2_analysis.json
      if: always()

  analyze-batch-3:
    needs: prepare-batches
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        pip install requests pandas numpy yfinance beautifulsoup4 lxml
    
    - name: Download batch files
      uses: actions/download-artifact@v4
      with:
        name: batch-inputs
        path: batch_inputs/
    
    - name: Run comprehensive analysis
      env:
        POLYGON_API_KEY: ${{ secrets.POLYGON_API_KEY }}
      run: |
        echo "ðŸ”¬ Analyzing Batch 3..."
        mkdir -p Verified_Backtest_Data
        
        if [ -f "phase3_comprehensive_collector_FIXED.py" ]; then
          python phase3_comprehensive_collector_FIXED.py batch3 batch_inputs/batch3_stocks.json
        elif [ -f "phase3_comprehensive_collector.py" ]; then
          python phase3_comprehensive_collector.py batch3 batch_inputs/batch3_stocks.json
        else
          echo '{"results": []}' > Verified_Backtest_Data/phase3_batch_batch3_analysis.json
        fi
    
    - name: Upload results
      uses: actions/upload-artifact@v4
      with:
        name: batch3-results
        path: Verified_Backtest_Data/phase3_batch_batch3_analysis.json
      if: always()

  analyze-batch-4:
    needs: prepare-batches
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        pip install requests pandas numpy yfinance beautifulsoup4 lxml
    
    - name: Download batch files
      uses: actions/download-artifact@v4
      with:
        name: batch-inputs
        path: batch_inputs/
    
    - name: Run comprehensive analysis
      env:
        POLYGON_API_KEY: ${{ secrets.POLYGON_API_KEY }}
      run: |
        echo "ðŸ”¬ Analyzing Batch 4..."
        mkdir -p Verified_Backtest_Data
        
        if [ -f "phase3_comprehensive_collector_FIXED.py" ]; then
          python phase3_comprehensive_collector_FIXED.py batch4 batch_inputs/batch4_stocks.json
        elif [ -f "phase3_comprehensive_collector.py" ]; then
          python phase3_comprehensive_collector.py batch4 batch_inputs/batch4_stocks.json
        else
          echo '{"results": []}' > Verified_Backtest_Data/phase3_batch_batch4_analysis.json
        fi
    
    - name: Upload results
      uses: actions/upload-artifact@v4
      with:
        name: batch4-results
        path: Verified_Backtest_Data/phase3_batch_batch4_analysis.json
      if: always()

  analyze-batch-5:
    needs: prepare-batches
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        pip install requests pandas numpy yfinance beautifulsoup4 lxml
    
    - name: Download batch files
      uses: actions/download-artifact@v4
      with:
        name: batch-inputs
        path: batch_inputs/
    
    - name: Run comprehensive analysis
      env:
        POLYGON_API_KEY: ${{ secrets.POLYGON_API_KEY }}
      run: |
        echo "ðŸ”¬ Analyzing Batch 5..."
        mkdir -p Verified_Backtest_Data
        
        if [ -f "phase3_comprehensive_collector_FIXED.py" ]; then
          python phase3_comprehensive_collector_FIXED.py batch5 batch_inputs/batch5_stocks.json
        elif [ -f "phase3_comprehensive_collector.py" ]; then
          python phase3_comprehensive_collector.py batch5 batch_inputs/batch5_stocks.json
        else
          echo '{"results": []}' > Verified_Backtest_Data/phase3_batch_batch5_analysis.json
        fi
    
    - name: Upload results
      uses: actions/upload-artifact@v4
      with:
        name: batch5-results
        path: Verified_Backtest_Data/phase3_batch_batch5_analysis.json
      if: always()

  merge-and-analyze:
    needs: [analyze-batch-1, analyze-batch-2, analyze-batch-3, analyze-batch-4, analyze-batch-5]
    if: always()
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        pip install pandas numpy
    
    - name: Create results directory
      run: |
        mkdir -p batch_results
        mkdir -p Verified_Backtest_Data
    
    - name: Download batch 1
      uses: actions/download-artifact@v4
      with:
        name: batch1-results
        path: batch_results/
      continue-on-error: true
    
    - name: Download batch 2
      uses: actions/download-artifact@v4
      with:
        name: batch2-results
        path: batch_results/
      continue-on-error: true
    
    - name: Download batch 3
      uses: actions/download-artifact@v4
      with:
        name: batch3-results
        path: batch_results/
      continue-on-error: true
    
    - name: Download batch 4
      uses: actions/download-artifact@v4
      with:
        name: batch4-results
        path: batch_results/
      continue-on-error: true
    
    - name: Download batch 5
      uses: actions/download-artifact@v4
      with:
        name: batch5-results
        path: batch_results/
      continue-on-error: true
    
    - name: Check and merge results
      run: |
        echo "ðŸ“ Checking batch_results directory..."
        
        if [ -z "$(ls -A batch_results 2>/dev/null)" ]; then
          echo "âŒ No batch results found!"
          echo "Creating empty merged file..."
          echo '{"merge_date": "'$(date -Iseconds)'", "total_stocks": 0, "all_stocks": []}' > Verified_Backtest_Data/phase3_merged_analysis.json
        else
          echo "âœ… Found results:"
          ls -lh batch_results/
          
          # Merge using v2 merger if available
          if [ -f "phase3_batch_merger_v2.py" ]; then
            echo "Using v2 merger"
            python phase3_batch_merger_v2.py batch_results/
          elif [ -f "phase3_batch_merger.py" ]; then
            echo "Using original merger"
            python phase3_batch_merger.py batch_results/
          else
            echo "Creating simple merger..."
            cat > simple_merger.py << 'MERGER'
        import json
        import glob
        from datetime import datetime
        
        files = glob.glob('batch_results/*.json')
        print(f"Merging {len(files)} files")
        
        all_stocks = []
        successful = 0
        failed = 0
        
        for file in files:
            try:
                with open(file) as f:
                    data = json.load(f)
                    if 'results' in data:
                        all_stocks.extend(data['results'])
                        successful += data.get('successful_analyses', 0)
                        failed += data.get('failed_analyses', 0)
            except Exception as e:
                print(f"Error reading {file}: {e}")
        
        with open('Verified_Backtest_Data/phase3_merged_analysis.json', 'w') as f:
            json.dump({
                'merge_date': datetime.now().isoformat(),
                'total_stocks': len(all_stocks),
                'successful_analyses': successful,
                'failed_analyses': failed,
                'all_stocks': all_stocks
            }, f, indent=2)
        
        print(f"Merged {len(all_stocks)} stocks")
        print(f"Successful: {successful}, Failed: {failed}")
        MERGER
            python simple_merger.py
          fi
        fi
    
    - name: Run correlation analysis
      run: |
        if [ -f "Verified_Backtest_Data/phase3_merged_analysis.json" ]; then
          echo "Running correlation analysis..."
          
          if [ -f "phase3_correlation_analyzer_v2.py" ]; then
            python phase3_correlation_analyzer_v2.py Verified_Backtest_Data/phase3_merged_analysis.json
          elif [ -f "phase3_correlation_analyzer.py" ]; then
            python phase3_correlation_analyzer.py Verified_Backtest_Data/phase3_merged_analysis.json
          else
            echo "No correlation analyzer found"
          fi
        else
          echo "No merged data to analyze"
        fi
    
    - name: Display summary
      run: |
        echo "## ðŸ“Š Phase 3 Analysis Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Mode**: ${{ github.event.inputs.mode }}" >> $GITHUB_STEP_SUMMARY
        echo "**Status**: Completed" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        if [ -f "Verified_Backtest_Data/phase3_merged_analysis.json" ]; then
          python << 'SUMMARY'
        import json
        with open('Verified_Backtest_Data/phase3_merged_analysis.json') as f:
            data = json.load(f)
        print(f"**Total Stocks Analyzed**: {data.get('total_stocks', 0)}")
        print(f"**Successful Analyses**: {data.get('successful_analyses', 0)}")
        print(f"**Failed Analyses**: {data.get('failed_analyses', 0)}")
        SUMMARY >> $GITHUB_STEP_SUMMARY
        else
          echo "No results to display" >> $GITHUB_STEP_SUMMARY
        fi
    
    - name: Commit results
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "Phase 3 Bot"
        
        git add Verified_Backtest_Data/*.json 2>/dev/null || true
        git add batch_inputs/ 2>/dev/null || true
        
        git diff --staged --quiet || git commit -m "ðŸ”¬ Phase 3 Analysis: ${{ github.event.inputs.mode }} mode
        
        Analysis complete with Polygon.io Stocks Advanced data
        Date: $(date +'%Y-%m-%d %H:%M')"
        
        git pull origin main --rebase --autostash || true
    
    - name: Push changes
      uses: ad-m/github-push-action@master
      with:
        github_token: ${{ secrets.GITHUB_TOKEN }}
        branch: main
      continue-on-error: true
