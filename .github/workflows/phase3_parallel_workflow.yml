name: Phase 3 Parallel Analysis - Complete Fixed

on:
  workflow_dispatch:
    inputs:
      mode:
        description: 'Analysis mode'
        required: true
        default: 'test'
        type: choice
        options:
          - test       # 10 stocks (5 batches √ó 2 stocks)
          - sample     # 100 stocks (5 batches √ó 20 stocks)
          - full       # All 694 stocks (10 batches √ó ~70 stocks)

permissions:
  contents: write

jobs:
  prepare-batches:
    runs-on: ubuntu-latest
    outputs:
      batch_count: ${{ steps.split.outputs.batch_count }}
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Debug - Check JSON structure
      run: |
        echo "üîç Checking explosive_stocks_CLEAN.json structure..."
        head -n 20 Verified_Backtest_Data/explosive_stocks_CLEAN.json || echo "File not found"
        
        python << 'CHECK'
        import json
        
        try:
            with open('Verified_Backtest_Data/explosive_stocks_CLEAN.json', 'r') as f:
                data = json.load(f)
            
            print(f"Type of loaded data: {type(data)}")
            
            if isinstance(data, dict):
                print(f"Keys in dictionary: {list(data.keys())[:5]}")
                # Check if there's a 'stocks' key or similar
                for key in ['stocks', 'results', 'data', 'explosive_stocks']:
                    if key in data:
                        print(f"Found '{key}' key with {len(data[key])} items")
            elif isinstance(data, list):
                print(f"Data is a list with {len(data)} items")
                if data:
                    print(f"First item type: {type(data[0])}")
        except Exception as e:
            print(f"Error checking file: {e}")
        CHECK
    
    - name: Split stocks into batches
      id: split
      env:
        MODE: ${{ github.event.inputs.mode }}
      run: |
        cat > batch_splitter_fixed.py << 'SCRIPT'
        import json
        import os
        import sys
        
        # Get mode from environment
        mode = os.environ.get('MODE', 'test')
        print(f"Running in {mode} mode")
        
        # Load explosive stocks - handle both list and dict formats
        try:
            with open('Verified_Backtest_Data/explosive_stocks_CLEAN.json', 'r') as f:
                data = json.load(f)
            
            # Handle different possible data structures
            if isinstance(data, list):
                # Data is already a list of stocks
                all_stocks = data
                print(f"Loaded {len(all_stocks)} stocks from list")
            elif isinstance(data, dict):
                # Data is a dictionary, find the stocks
                # Try common keys
                possible_keys = ['stocks', 'results', 'data', 'explosive_stocks', 'all_stocks']
                all_stocks = None
                
                for key in possible_keys:
                    if key in data and isinstance(data[key], list):
                        all_stocks = data[key]
                        print(f"Found stocks under '{key}' key: {len(all_stocks)} stocks")
                        break
                
                if all_stocks is None:
                    # If no standard key, try to find any list value
                    for key, value in data.items():
                        if isinstance(value, list) and len(value) > 0:
                            # Check if it looks like stock data
                            if isinstance(value[0], dict) and any(k in value[0] for k in ['ticker', 'symbol']):
                                all_stocks = value
                                print(f"Found stocks under '{key}' key: {len(all_stocks)} stocks")
                                break
                
                if all_stocks is None:
                    print("ERROR: Could not find stock list in JSON file")
                    print(f"Available keys: {list(data.keys())}")
                    sys.exit(1)
            else:
                print(f"ERROR: Unexpected data type: {type(data)}")
                sys.exit(1)
                
        except FileNotFoundError:
            print("ERROR: explosive_stocks_CLEAN.json not found")
            sys.exit(1)
        except json.JSONDecodeError as e:
            print(f"ERROR: Invalid JSON in file: {e}")
            sys.exit(1)
        except Exception as e:
            print(f"ERROR: Unexpected error: {e}")
            sys.exit(1)
        
        # Determine batch parameters
        if mode == "test":
            stocks_to_process = all_stocks[:10] if len(all_stocks) >= 10 else all_stocks
            batch_count = min(5, len(stocks_to_process))
            print(f"TEST MODE: Processing {len(stocks_to_process)} stocks in {batch_count} batches")
        elif mode == "sample":
            stocks_to_process = all_stocks[:100] if len(all_stocks) >= 100 else all_stocks
            batch_count = min(5, len(stocks_to_process))
            print(f"SAMPLE MODE: Processing {len(stocks_to_process)} stocks in {batch_count} batches")
        else:  # full
            stocks_to_process = all_stocks
            batch_count = min(10, len(stocks_to_process))
            print(f"FULL MODE: Processing {len(stocks_to_process)} stocks in {batch_count} batches")
        
        # Create batch_inputs directory
        os.makedirs('batch_inputs', exist_ok=True)
        
        # Calculate batch size
        total_stocks = len(stocks_to_process)
        
        if total_stocks == 0:
            print("ERROR: No stocks to process")
            sys.exit(1)
        
        batch_size = total_stocks // batch_count
        remainder = total_stocks % batch_count
        
        # Split into batches
        start_idx = 0
        batches_created = 0
        
        for i in range(batch_count):
            # Add extra stock to earlier batches if there's a remainder
            current_batch_size = batch_size + (1 if i < remainder else 0)
            end_idx = start_idx + current_batch_size
            
            if end_idx > total_stocks:
                end_idx = total_stocks
            
            batch_stocks = stocks_to_process[start_idx:end_idx]
            
            if batch_stocks:
                batch_num = i + 1
                batch_file = f'batch_inputs/batch{batch_num}_stocks.json'
                
                batch_data = {
                    'batch': batch_num,
                    'stocks': batch_stocks
                }
                
                with open(batch_file, 'w') as f:
                    json.dump(batch_data, f, indent=2)
                
                print(f"Created batch {batch_num}: {len(batch_stocks)} stocks")
                batches_created += 1
            
            start_idx = end_idx
            
            if start_idx >= total_stocks:
                break
        
        # Write output for GitHub Actions
        with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
            f.write(f"batch_count={batches_created}\n")
        
        print(f"Successfully created {batches_created} batches")
        SCRIPT
        
        python batch_splitter_fixed.py
        
        echo ""
        echo "üìÅ Batch files created:"
        ls -la batch_inputs/ || echo "No batch_inputs directory created"
    
    - name: Upload batch files
      uses: actions/upload-artifact@v4
      with:
        name: batch-inputs
        path: batch_inputs/

  analyze-batch-1:
    needs: prepare-batches
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        pip install requests pandas numpy yfinance beautifulsoup4 lxml
    
    - name: Download batch files
      uses: actions/download-artifact@v4
      with:
        name: batch-inputs
        path: batch_inputs/
    
    - name: Run comprehensive analysis
      env:
        POLYGON_API_KEY: ${{ secrets.POLYGON_API_KEY }}
      run: |
        echo "üî¨ Analyzing Batch 1..."
        
        # Create output directory
        mkdir -p Verified_Backtest_Data
        
        # Try to run the collector
        if [ -f "phase3_comprehensive_collector_FIXED.py" ]; then
          echo "Using comprehensive collector FIXED"
          python phase3_comprehensive_collector_FIXED.py batch1 batch_inputs/batch1_stocks.json
        elif [ -f "phase3_comprehensive_collector.py" ]; then
          echo "Using original collector"
          python phase3_comprehensive_collector.py batch1 batch_inputs/batch1_stocks.json
        else
          echo "No collector found - creating minimal output"
          cat > minimal_collector.py << 'COLLECTOR'
        import json
        import sys
        from datetime import datetime
        
        batch_name = sys.argv[1] if len(sys.argv) > 1 else "batch1"
        batch_file = sys.argv[2] if len(sys.argv) > 2 else "batch_inputs/batch1_stocks.json"
        
        with open(batch_file, 'r') as f:
            batch_data = json.load(f)
        
        results = []
        for stock in batch_data.get('stocks', []):
            results.append({
                'ticker': stock.get('ticker'),
                'gain_percent': stock.get('gain_percent'),
                'analysis_status': 'minimal',
                'total_score_pre': 0
            })
        
        output = {
            'batch_name': batch_name,
            'analysis_date': datetime.now().isoformat(),
            'successful_analyses': len(results),
            'failed_analyses': 0,
            'results': results
        }
        
        with open(f'Verified_Backtest_Data/phase3_batch_{batch_name}_analysis.json', 'w') as f:
            json.dump(output, f, indent=2)
        print(f"Created minimal output with {len(results)} stocks")
        COLLECTOR
          python minimal_collector.py batch1 batch_inputs/batch1_stocks.json
        fi
        
        # Verify output exists
        if [ -f "Verified_Backtest_Data/phase3_batch_batch1_analysis.json" ]; then
          echo "‚úÖ Output file created successfully"
        else
          echo "‚ùå No output file created!"
          echo '{"results": []}' > Verified_Backtest_Data/phase3_batch_batch1_analysis.json
        fi
    
    - name: Upload results
      uses: actions/upload-artifact@v4
      with:
        name: batch1-results
        path: Verified_Backtest_Data/phase3_batch_batch1_analysis.json
      if: always()

  analyze-batch-2:
    needs: prepare-batches
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        pip install requests pandas numpy yfinance beautifulsoup4 lxml
    
    - name: Download batch files
      uses: actions/download-artifact@v4
      with:
        name: batch-inputs
        path: batch_inputs/
    
    - name: Run comprehensive analysis
      env:
        POLYGON_API_KEY: ${{ secrets.POLYGON_API_KEY }}
      run: |
        echo "üî¨ Analyzing Batch 2..."
        mkdir -p Verified_Backtest_Data
        
        if [ -f "phase3_comprehensive_collector_FIXED.py" ]; then
          python phase3_comprehensive_collector_FIXED.py batch2 batch_inputs/batch2_stocks.json
        elif [ -f "phase3_comprehensive_collector.py" ]; then
          python phase3_comprehensive_collector.py batch2 batch_inputs/batch2_stocks.json
        else
          echo '{"results": []}' > Verified_Backtest_Data/phase3_batch_batch2_analysis.json
        fi
    
    - name: Upload results
      uses: actions/upload-artifact@v4
      with:
        name: batch2-results
        path: Verified_Backtest_Data/phase3_batch_batch2_analysis.json
      if: always()

  analyze-batch-3:
    needs: prepare-batches
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        pip install requests pandas numpy yfinance beautifulsoup4 lxml
    
    - name: Download batch files
      uses: actions/download-artifact@v4
      with:
        name: batch-inputs
        path: batch_inputs/
    
    - name: Run comprehensive analysis
      env:
        POLYGON_API_KEY: ${{ secrets.POLYGON_API_KEY }}
      run: |
        echo "üî¨ Analyzing Batch 3..."
        mkdir -p Verified_Backtest_Data
        
        if [ -f "phase3_comprehensive_collector_FIXED.py" ]; then
          python phase3_comprehensive_collector_FIXED.py batch3 batch_inputs/batch3_stocks.json
        elif [ -f "phase3_comprehensive_collector.py" ]; then
          python phase3_comprehensive_collector.py batch3 batch_inputs/batch3_stocks.json
        else
          echo '{"results": []}' > Verified_Backtest_Data/phase3_batch_batch3_analysis.json
        fi
    
    - name: Upload results
      uses: actions/upload-artifact@v4
      with:
        name: batch3-results
        path: Verified_Backtest_Data/phase3_batch_batch3_analysis.json
      if: always()

  analyze-batch-4:
    needs: prepare-batches
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        pip install requests pandas numpy yfinance beautifulsoup4 lxml
    
    - name: Download batch files
      uses: actions/download-artifact@v4
      with:
        name: batch-inputs
        path: batch_inputs/
    
    - name: Run comprehensive analysis
      env:
        POLYGON_API_KEY: ${{ secrets.POLYGON_API_KEY }}
      run: |
        echo "üî¨ Analyzing Batch 4..."
        mkdir -p Verified_Backtest_Data
        
        if [ -f "phase3_comprehensive_collector_FIXED.py" ]; then
          python phase3_comprehensive_collector_FIXED.py batch4 batch_inputs/batch4_stocks.json
        elif [ -f "phase3_comprehensive_collector.py" ]; then
          python phase3_comprehensive_collector.py batch4 batch_inputs/batch4_stocks.json
        else
          echo '{"results": []}' > Verified_Backtest_Data/phase3_batch_batch4_analysis.json
        fi
    
    - name: Upload results
      uses: actions/upload-artifact@v4
      with:
        name: batch4-results
        path: Verified_Backtest_Data/phase3_batch_batch4_analysis.json
      if: always()

  analyze-batch-5:
    needs: prepare-batches
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        pip install requests pandas numpy yfinance beautifulsoup4 lxml
    
    - name: Download batch files
      uses: actions/download-artifact@v4
      with:
        name: batch-inputs
        path: batch_inputs/
    
    - name: Run comprehensive analysis
      env:
        POLYGON_API_KEY: ${{ secrets.POLYGON_API_KEY }}
      run: |
        echo "üî¨ Analyzing Batch 5..."
        mkdir -p Verified_Backtest_Data
        
        if [ -f "phase3_comprehensive_collector_FIXED.py" ]; then
          python phase3_comprehensive_collector_FIXED.py batch5 batch_inputs/batch5_stocks.json
        elif [ -f "phase3_comprehensive_collector.py" ]; then
          python phase3_comprehensive_collector.py batch5 batch_inputs/batch5_stocks.json
        else
          echo '{"results": []}' > Verified_Backtest_Data/phase3_batch_batch5_analysis.json
        fi
    
    - name: Upload results
      uses: actions/upload-artifact@v4
      with:
        name: batch5-results
        path: Verified_Backtest_Data/phase3_batch_batch5_analysis.json
      if: always()

  merge-and-analyze:
    needs: [analyze-batch-1, analyze-batch-2, analyze-batch-3, analyze-batch-4, analyze-batch-5]
    if: always()
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        pip install pandas numpy
    
    - name: Create results directory
      run: |
        mkdir -p batch_results
        mkdir -p Verified_Backtest_Data
    
    - name: Download batch 1
      uses: actions/download-artifact@v4
      with:
        name: batch1-results
        path: batch_results/
      continue-on-error: true
    
    - name: Download batch 2
      uses: actions/download-artifact@v4
      with:
        name: batch2-results
        path: batch_results/
      continue-on-error: true
    
    - name: Download batch 3
      uses: actions/download-artifact@v4
      with:
        name: batch3-results
        path: batch_results/
      continue-on-error: true
    
    - name: Download batch 4
      uses: actions/download-artifact@v4
      with:
        name: batch4-results
        path: batch_results/
      continue-on-error: true
    
    - name: Download batch 5
      uses: actions/download-artifact@v4
      with:
        name: batch5-results
        path: batch_results/
      continue-on-error: true
    
    - name: Check and merge results
      run: |
        echo "üìÅ Checking batch_results directory..."
        
        if [ -z "$(ls -A batch_results 2>/dev/null)" ]; then
          echo "‚ùå No batch results found!"
          echo "Creating empty merged file..."
          echo '{"merge_date": "'$(date -Iseconds)'", "total_stocks": 0, "all_stocks": []}' > Verified_Backtest_Data/phase3_merged_analysis.json
        else
          echo "‚úÖ Found results:"
          ls -lh batch_results/
          
          # Merge using v2 merger if available
          if [ -f "phase3_batch_merger_v2.py" ]; then
            echo "Using v2 merger"
            python phase3_batch_merger_v2.py batch_results/
          elif [ -f "phase3_batch_merger.py" ]; then
            echo "Using original merger"
            python phase3_batch_merger.py batch_results/
          else
            echo "Creating simple merger..."
            cat > simple_merger.py << 'MERGESCRIPT'
        import json
        import glob
        from datetime import datetime
        
        files = glob.glob('batch_results/*.json')
        print(f"Merging {len(files)} files")
        
        all_stocks = []
        successful = 0
        failed = 0
        
        for file in files:
            try:
                with open(file) as f:
                    data = json.load(f)
                    if 'results' in data:
                        all_stocks.extend(data['results'])
                        successful += data.get('successful_analyses', 0)
                        failed += data.get('failed_analyses', 0)
            except Exception as e:
                print(f"Error reading {file}: {e}")
        
        os.makedirs('Verified_Backtest_Data', exist_ok=True)
        with open('Verified_Backtest_Data/phase3_merged_analysis.json', 'w') as f:
            json.dump({
                'merge_date': datetime.now().isoformat(),
                'total_stocks': len(all_stocks),
                'successful_analyses': successful,
                'failed_analyses': failed,
                'all_stocks': all_stocks
            }, f, indent=2)
        
        print(f"Merged {len(all_stocks)} stocks")
        print(f"Successful: {successful}, Failed: {failed}")
        MERGESCRIPT
            python simple_merger.py
          fi
        fi
    
    - name: Run correlation analysis
      run: |
        if [ -f "Verified_Backtest_Data/phase3_merged_analysis.json" ]; then
          echo "Running correlation analysis..."
          
          if [ -f "phase3_correlation_analyzer_v2.py" ]; then
            python phase3_correlation_analyzer_v2.py Verified_Backtest_Data/phase3_merged_analysis.json
          elif [ -f "phase3_correlation_analyzer.py" ]; then
            python phase3_correlation_analyzer.py Verified_Backtest_Data/phase3_merged_analysis.json
          else
            echo "No correlation analyzer found"
          fi
        else
          echo "No merged data to analyze"
        fi
    
    - name: Display summary
      run: |
        echo "## üìä Phase 3 Analysis Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Mode**: ${{ github.event.inputs.mode }}" >> $GITHUB_STEP_SUMMARY
        echo "**Status**: Completed" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        if [ -f "Verified_Backtest_Data/phase3_merged_analysis.json" ]; then
          cat > summary_generator.py << 'PYTHONSCRIPT'
        import json
        try:
            with open('Verified_Backtest_Data/phase3_merged_analysis.json') as f:
                data = json.load(f)
            print(f"**Total Stocks Analyzed**: {data.get('total_stocks', 0)}")
            print(f"**Successful Analyses**: {data.get('successful_analyses', 0)}")
            print(f"**Failed Analyses**: {data.get('failed_analyses', 0)}")
        except Exception as e:
            print(f"Error reading results: {e}")
        PYTHONSCRIPT
          python summary_generator.py >> $GITHUB_STEP_SUMMARY
        else
          echo "No results to display" >> $GITHUB_STEP_SUMMARY
        fi
    
    - name: Commit results
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "Phase 3 Bot"
        
        git add Verified_Backtest_Data/*.json 2>/dev/null || true
        git add batch_inputs/ 2>/dev/null || true
        
        git diff --staged --quiet || git commit -m "üî¨ Phase 3 Analysis: ${{ github.event.inputs.mode }} mode
        
        Analysis complete with Polygon.io Stocks Advanced data
        Date: $(date +'%Y-%m-%d %H:%M')"
        
        git pull origin main --rebase --autostash || true
    
    - name: Push changes
      uses: ad-m/github-push-action@master
      with:
        github_token: ${{ secrets.GITHUB_TOKEN }}
        branch: main
      continue-on-error: true
